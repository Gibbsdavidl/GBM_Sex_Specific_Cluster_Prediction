---
title: An R Markdown document converted from "STEP_4_AIM_1_CV_Pairs.ipynb"
output: html_document
---

Changes:

* Subsetting to genes that are present in all data sources.. array, RNA-seq, and TEMPUS. 10363 genes.
* Subsetting the JIVE gene signatures to those present genes.

Questions: Are there missing genes, in array, that are present in RNA-seq, that would be better to have for TEMPUS?



```{r}
devtools::install_github("gibbsdavidl/robencla", force = T)
```

```{r}
library(tidyverse)
library(robencla)
library(pheatmap)
```

```{r}

# pairlist is a list of vectors (paired terms)
# n is the number of pairs for each entry desired

shorter_plist <- function(pairlist, n) {
  newlist <- list()
  for (ni in names(pairlist)) {
    newlist[[ni]] <- pairlist[[ni]][1:(2*n)]
  }
  return(newlist)  
}


```



```{r}
dat.m <- read_csv('../data/jive_training_array_data_M.csv.gz')

load("../results/males_genepairs.rda")

# removing signature genes that are not available in our data sets
male.jive.gene.list <- readRDS("../data/Males_jive_cluster_genes.rds") %>% dplyr::filter(value %in% colnames(dat.m))

dat.m <- dat.m[,-1]
```

```{r}
table(dat.m$ClusterLabel)
```

```{r}
dim(dat.m)
print(tail(colnames(dat.m)))
```


```{r}

sigs.m <- list("C1"=male.jive.gene.list$value[male.jive.gene.list$cluster.group == 'cluster1'],
               "C2"=male.jive.gene.list$value[male.jive.gene.list$cluster.group == 'cluster2'],
               "C3"=male.jive.gene.list$value[male.jive.gene.list$cluster.group == 'cluster3'],
               "C4"=male.jive.gene.list$value[male.jive.gene.list$cluster.group == 'cluster4'],
               "C5"=male.jive.gene.list$value[male.jive.gene.list$cluster.group == 'cluster5']
)

# in case we want to test it #
idx <- sample(1:220, size=140, replace = F)
jdx <- setdiff(1:220, idx)
train <- dat.m[idx,1:ncol(dat.m)]
test <- dat.m[jdx,1:ncol(dat.m)]

# our classifier object named Roberta
mod_m <- Robencla$new("male models")

# xgboost parameters to pass to each sub-classifier in the ensembles
params <- list(max_depth=12,   # "height" of the tree, 6 is actually default. I think about 12 seems better.  (xgboost parameter)
               eta=0.45,        # this is the learning rate. smaller values slow it down, more  conservative   (xgboost parameter)
               nrounds=24,     # number of rounds of training, lower numbers less overfitting (potentially)  (xgboost parameter)
               early_stopping_rounds=2,
               nthreads=8,     # parallel threads
               gamma=0.2,      # Minimum loss req'd to again partition a leaf node. higher number ~ more conservative (xgboost)
               lambda=2.5,     # L2 regularization term on weights, higher number ~ more conservative (xgboost parameter)
               alpha=0.25,      # L1 regularization term on weights. higher number ~ more conservative (xgboost parameter)
               verbose=0,
               train_perc=0.8,
               combine_function='median',
               size=11
)

gpairs <- shorter_plist(genepairs, 5)

mod_m$train(data_frame=train,
            label_name='ClusterLabel',
            sample_id = 'Barcode',
            drop_list = 'Sex',
            data_mode=c('pairs'), # pairs, allpairs, sigpairs, quartiles, tertiles, binarize, ranks, original #
            signatures=NULL,
            pair_list=genepairs,  # subset to these genes.
            params=params
            )

mod_m$predict(data_frame = test, 
              label_name = 'ClusterLabel', 
              sample_id = 'Barcode')#,
              #drop_list = 'Sex')

table(mod_m$test_label, mod_m$results()$BestCall)

mod_m$importance()
```


```{r}


# our classifier object named Roberta
mod_m <- Robencla$new("mod_m")


# xgboost parameters to pass to each sub-classifier in the ensembles
params <- list(max_depth=12,   # "height" of the tree, 6 is actually default. I think about 12 seems better.  (xgboost parameter)
               eta=0.45,        # this is the learning rate. smaller values slow it down, more  conservative   (xgboost parameter)
               nrounds=24,     # number of rounds of training, lower numbers less overfitting (potentially)  (xgboost parameter)
               early_stopping_rounds=2,
               nthreads=8,     # parallel threads
               gamma=0.2,      # Minimum loss req'd to again partition a leaf node. higher number ~ more conservative (xgboost)
               lambda=2.5,     # L2 regularization term on weights, higher number ~ more conservative (xgboost parameter)
               alpha=0.25,      # L1 regularization term on weights. higher number ~ more conservative (xgboost parameter)
               verbose=0,
               train_perc=0.8,
               combine_function='median',
               size=11
)

gpairs <- shorter_plist(genepairs, 10)

# First we use the training data
mod_m$autocv(data_frame=dat.m,
            label_name='ClusterLabel',
            sample_id = 'Barcode',
            drop_list='Sex',
            data_mode=c('pairs'), # pairs, allpairs, sigpairs, quartiles, tertiles, binarize, ranks, original #
            signatures=NULL, #sigs.m,
            pair_list=gpairs,  # subset to these genes.
            params=params,
            cv_rounds=10
            )
```

```{r}
df.m <- mod_m$cv_results
df.m$cluster1 <- as.numeric(df.m$cluster1)
df.m$cluster2 <- as.numeric(df.m$cluster2)
df.m$cluster3 <- as.numeric(df.m$cluster3)
df.m$cluster4 <- as.numeric(df.m$cluster4)
df.m$cluster5 <- as.numeric(df.m$cluster5)
head(df.m)

#write.table(as.data.frame(df.m), '../results/male_tcga_cv_pairs_and_sigpairs.csv', sep=',', quote = F, row.names = F)
#write.table(as.data.frame(df.m), '../results/male_tcga_cv_pairs_only.csv', sep=',', quote = F, row.names = F)
```

```{r}


save_pheatmap_pdf <- function(x, filename, width=7, height=7) {
    stopifnot(!missing(x))
    stopifnot(!missing(filename))
    pdf(filename, width=width, height=height)
    grid::grid.newpage()
    grid::grid.draw(x$gtable)
    dev.off()
}

tabdf <- table(Pred=df.m$BestCalls, True=df.m$Label)

print(tabdf)

tabtbl <- as.data.frame(tabdf) %>% 
  pivot_wider(id_cols = Pred,names_from = True, values_from = Freq) %>% 
  select(-Pred) %>%
  scale(center = F) # Z-scores
rownames(tabtbl) <- colnames(tabtbl)

p <- pheatmap(tabtbl, cluster_rows = F, cluster_cols = F)
save_pheatmap_pdf(p, "../figures/males_TCGA_CV_heatmap.pdf", width=5, height=5)

```




```{r}
metrics <- mod_m$classification_metrics()
write_csv(metrics, file='../results/males_tcga_array_metrics_step4.csv')
metrics
```

```{r}

imp_list <- mod_m$importance()

save(imp_list, file='../results/male_CV_pairs_importance_step4.rda')

imp_list
```

```{r}

df1 <- data.frame(Top10=1:10, Cluster='cluster1',imp_list[['cluster1']][1:10,])
df2 <- data.frame(Top10=1:10, Cluster='cluster2',imp_list[['cluster2']][1:10,])
df3 <- data.frame(Top10=1:10, Cluster='cluster3',imp_list[['cluster3']][1:10,])
df4 <- data.frame(Top10=1:10, Cluster='cluster4',imp_list[['cluster4']][1:10,])
df5 <- data.frame(Top10=1:10, Cluster='cluster5',imp_list[['cluster5']][1:10,])

impdf <- rbind(df1[,c(1,2,4)],df2[,c(1,2,4)],df3[,c(1,2,4)],df4[,c(1,2,4)],df5[,c(1,2,4)])

colnames(impdf) <- c('Top10','Cluster','Med_Info_Gain')



#ggplot(impdf, aes(Top15)) + 
#  geom_line(aes(y=cluster1),color="black") +
#  geom_line(aes(y=cluster2),color="orange") +
#  geom_line(aes(y=cluster3),color="skyblue") +
#  geom_line(aes(y=cluster4),color="darkgreen") +
#  geom_line(aes(y=cluster5),color="red") +
#  xlab('Top 15 gene pairs') + 
#  ylab('Median information gain')

ggplot(impdf, aes(x=Top10,y=Med_Info_Gain,color=Cluster)) + geom_line() + 
  xlab("Top 10 features per cluster") + ylab("Median Information Gain")
ggsave('../figures/males_TCGA_CV_feature_importance.pdf')

allimp <- rbind(df1,df2,df3,df4,df5)

write.csv(allimp, file='../results/male_importance_table.csv')

```




```{r}
#ensemble_rocs(anne)
plot_roc(mod_m, 1, flip=T)
```

```{r}
#ensemble_rocs(anne)
plot_roc(mod_m, 2, flip=T)
```

```{r}
#ensemble_rocs(anne)
plot_roc(mod_m, 3, flip=T)
```

```{r}
#ensemble_rocs(anne)
plot_roc(mod_m, 4, flip=F)
```

```{r}
#ensemble_rocs(anne)
plot_roc(mod_m, 5, flip=F)
```

```{r}
save(mod_m, file='../models/males_tcga_array_step4.rda')
```






```{r}





### FEMALES ###








```







```{r}
dat_f <- read_csv('../data/jive_training_array_data_F.csv.gz')

load('../results/females_genepairs.rda')

lapply(genepairs, function(x) x[! (x %in% colnames(dat_f))] )

# removing signature genes that are not available in our data sets
female.jive.gene.list <- readRDS("../data/Female_jive_cluster_genes.rds") %>% dplyr::filter(value %in% colnames(dat_f))

sigs_f <- list("C1"=female.jive.gene.list$value[female.jive.gene.list$cluster.group == 'cluster1'],
               "C2"=female.jive.gene.list$value[female.jive.gene.list$cluster.group == 'cluster2'],
               "C3"=female.jive.gene.list$value[female.jive.gene.list$cluster.group == 'cluster3'],
               "C4"=female.jive.gene.list$value[female.jive.gene.list$cluster.group == 'cluster4'],
               "C5"=female.jive.gene.list$value[female.jive.gene.list$cluster.group == 'cluster5']
)

dat_f <- dat_f[,-1]

```

```{r}
table(dat_f$ClusterLabel)
```

```{r}
lapply(genepairs, length)
```


```{r}

# in case we want to test it #
idx <- sample(1:139, size=100, replace = F)
jdx <- setdiff(1:139, idx)
train <- dat_f[idx,]
test <- dat_f[jdx,]

# our classifier object named Roberta
buffy <- Robencla$new("buffy")

# genes with dots in the symbol get transformed to _
#lapply(genepairs, function(x) x[! (x %in% colnames(buffy$train_data))] )
#genepairs$cluster5[25] <- "CTC-338M12_4"

# xgboost parameters to pass to each sub-classifier in the ensembles
params <- list(max_depth=12,   # "height" of the tree, 6 is actually default. I think about 12 seems better.  (xgboost parameter)
               eta=0.45,        # this is the learning rate. smaller values slow it down, more  conservative   (xgboost parameter)
               nrounds=24,     # number of rounds of training, lower numbers less overfitting (potentially)  (xgboost parameter)
               early_stopping_rounds=2,
               nthreads=8,     # parallel threads
               gamma=0.2,      # Minimum loss req'd to again partition a leaf node. higher number ~ more conservative (xgboost)
               lambda=2.5,     # L2 regularization term on weights, higher number ~ more conservative (xgboost parameter)
               alpha=0.25,      # L1 regularization term on weights. higher number ~ more conservative (xgboost parameter)
               verbose=0,
               train_perc=0.8,
               combine_function='median',
               size=11
)

gpairs <- shorter_plist(genepairs, 5)

buffy$train(data_frame=train,
            label_name='ClusterLabel',
            sample_id = 'Barcode',
            drop_list = 'Sex',
            data_mode='pairs', # pairs, allpairs, sigpairs, quartiles, tertiles, binarize, ranks, original #
            signatures=NULL,
            pair_list=gpairs,  # subset to these genes.
            params=params
            )

buffy$predict(data_frame = test, 
              label_name = 'ClusterLabel', 
              sample_id = 'Barcode',
              drop_list = 'Sex')

table(buffy$test_label, buffy$results()$BestCall)

```


```{r}

# our classifier object named Roberta
buffy <- Robencla$new("buffy")

# xgboost parameters to pass to each sub-classifier in the ensembles
params <- list(max_depth=12,   # "height" of the tree, 6 is actually default. I think about 12 seems better.  (xgboost parameter)
               eta=0.45,        # this is the learning rate. smaller values slow it down, more  conservative   (xgboost parameter)
               nrounds=24,     # number of rounds of training, lower numbers less overfitting (potentially)  (xgboost parameter)
               early_stopping_rounds=2,
               nthreads=12,     # parallel threads
               gamma=0.2,      # Minimum loss req'd to again partition a leaf node. higher number ~ more conservative (xgboost)
               lambda=2.5,     # L2 regularization term on weights, higher number ~ more conservative (xgboost parameter)
               alpha=0.25,      # L1 regularization term on weights. higher number ~ more conservative (xgboost parameter)
               verbose=0,
               train_perc=0.8,
               combine_function='median',
               size=11
)


glist <- shorter_plist(genepairs, 10)

# First we use the training data
buffy$autocv(data_frame=dat_f,
            label_name='ClusterLabel',
            sample_id = 'sample',
            drop_list = 'Sex',
            data_mode=c('pairs'), # pairs, allpairs, sigpairs, quartiles, tertiles, binarize, ranks, original #
            signatures=NULL,
            pair_list=glist,  # subset to these genes.
            params=params,
            cv_rounds=10
            )
```

```{r}
df.f <- buffy$cv_results
df.f$cluster1 <- as.numeric(df.f$cluster1)
df.f$cluster2 <- as.numeric(df.f$cluster2)
df.f$cluster3 <- as.numeric(df.f$cluster3)
df.f$cluster4 <- as.numeric(df.f$cluster4)
df.f$cluster5 <- as.numeric(df.f$cluster5)
head(df.f)

write.table(as.data.frame(df.f), '../results/female_cv_pairs_step4.csv', sep=',', quote = F, row.names = F)
```

```{r}

save_pheatmap_pdf <- function(x, filename, width=7, height=7) {
    stopifnot(!missing(x))
    stopifnot(!missing(filename))
    pdf(filename, width=width, height=height)
    grid::grid.newpage()
    grid::grid.draw(x$gtable)
    dev.off()
}

tabdf <- table(Pred=df.f$BestCalls, True=df.f$Label)

print(tabdf)

tabtbl <- as.data.frame(tabdf) %>% 
  pivot_wider(id_cols = Pred,names_from = True, values_from = Freq) %>% 
  select(-Pred) %>%
  scale(center = F) # Z-scores
rownames(tabtbl) <- colnames(tabtbl)

p <- pheatmap(tabtbl, cluster_rows = F, cluster_cols = F)

save_pheatmap_pdf(p, "../figures/female_TCGA_CV_heatmap.pdf")

```

```{r}
metrics <- buffy$classification_metrics()
write_csv(metrics, file='../results/females_tcga_array_metrics_step4.csv')
metrics
```

```{r}

imp_list <- buffy$importance()

save(imp_list, file='../results/female_CV_pairs_importance_step4.rda')

imp_list

```


```{r}

df1 <- data.frame(Top10=1:10, Cluster='cluster1',imp_list[['cluster1']][1:10,])
df2 <- data.frame(Top10=1:10, Cluster='cluster2',imp_list[['cluster2']][1:10,])
df3 <- data.frame(Top10=1:10, Cluster='cluster3',imp_list[['cluster3']][1:10,])
df4 <- data.frame(Top10=1:10, Cluster='cluster4',imp_list[['cluster4']][1:10,])
df5 <- data.frame(Top10=1:10, Cluster='cluster5',imp_list[['cluster5']][1:10,])

impdf <- rbind(df1[,c(1,2,4)],df2[,c(1,2,4)],df3[,c(1,2,4)],df4[,c(1,2,4)],df5[,c(1,2,4)])

colnames(impdf) <- c('Top10','Cluster','Med_Info_Gain')

#ggplot(impdf, aes(Top15)) + 
#  geom_line(aes(y=cluster1),color="black") +
#  geom_line(aes(y=cluster2),color="orange") +
#  geom_line(aes(y=cluster3),color="skyblue") +
#  geom_line(aes(y=cluster4),color="darkgreen") +
#  geom_line(aes(y=cluster5),color="red") +
#  xlab('Top 15 gene pairs') + 
#  ylab('Median information gain')

ggplot(impdf, aes(x=Top10,y=Med_Info_Gain,color=Cluster)) + geom_line() + 
  xlab("Top 10 features per cluster") + ylab("Median Information Gain")
ggsave("../figures/female_TCGA_CV_feature_importance.pdf")


load('../results/female_CV_pairs_importance_step4.rda')

allimp <- rbind(df1,df2,df3,df4,df5)

write.csv(allimp, file='../results/female_importance_table.csv')

```



```{r}
#ensemble_rocs(anne)
plot_roc(buffy, 1, flip=T)
```

```{r}
#ensemble_rocs(anne)
plot_roc(buffy, 2, flip=T)
```

```{r}
#ensemble_rocs(anne)
plot_roc(buffy, 3, flip=T)
```

```{r}
#ensemble_rocs(anne)
plot_roc(buffy, 4, flip=T)
```

```{r}
#ensemble_rocs(anne)
plot_roc(buffy, 5, flip=F)
```

```{r}

save(buffy, file='../models/females_tcga_array_step4.rda')

#FIN
```

